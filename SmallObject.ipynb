{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmallObject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuganyaMurugesan/TrafficLightDetection/blob/master/SmallObject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d38-m5kkz_NX",
        "colab_type": "code",
        "outputId": "1f377658-2db6-42f2-8e27-eb0d006d235f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qC5AZp40GaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/HCLHackathon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojmw9ZSD0myT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "25c95383-2887-46b6-eef7-7bfa0d1e01d5"
      },
      "source": [
        "\"\"\"\n",
        "Mask R-CNN\n",
        "Train on the toy Balloon dataset and implement color splash effect.\n",
        "Copyright (c) 2018 Matterport, Inc.\n",
        "Licensed under the MIT License (see LICENSE for details)\n",
        "Written by Waleed Abdulla\n",
        "------------------------------------------------------------\n",
        "Usage: import the module (see Jupyter notebooks for examples), or run from\n",
        "       the command line as such:\n",
        "    # Train a new model starting from pre-trained COCO weights\n",
        "    python3 balloon.py train --dataset=/path/to/balloon/dataset --weights=coco\n",
        "    # Resume training a model that you had trained earlier\n",
        "    python3 balloon.py train --dataset=/path/to/balloon/dataset --weights=last\n",
        "    # Train a new model starting from ImageNet weights\n",
        "    python3 balloon.py train --dataset=/path/to/balloon/dataset --weights=imagenet\n",
        "    # Apply color splash to an image\n",
        "    python3 balloon.py splash --weights=/path/to/weights/file.h5 --image=<URL or path to file>\n",
        "    # Apply color splash to video using the last weights you trained\n",
        "    python3 balloon.py splash --weights=last --video=<URL or path to file>\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import datetime\n",
        "import numpy as np\n",
        "import skimage.draw\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.getcwd()\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib, utils\n",
        "from mrcnn import visualize\n",
        "from mrcnn.visualize import display_images\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn.model import log\n",
        "\n",
        "#from samples.balloon import balloon\n",
        "\n",
        "%matplotlib inline\n",
        "# Path to trained weights file\n",
        "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "# Directory to save logs and model checkpoints, if not provided\n",
        "# through the command line argument --logs\n",
        "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "TRAFFICLIGHT_DIR = \"/content/drive/My Drive/Colab Notebooks/HCLHackathon/data\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-mWv7Gh0yWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrafficLightConfig(Config):\n",
        "    \"\"\"Configuration for training on the toy  dataset.\n",
        "    Derives from the base Config class and overrides some values.\n",
        "  \n",
        "      \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"TrafficLight\"\n",
        "\n",
        "    # Whether to use image augmentation in training mode\n",
        "    AUGMENT = True\n",
        "\n",
        "    # Whether to use image scaling and rotations in training mode\n",
        "    SCALE = True\n",
        "\n",
        "    # Optimizer, default is 'SGD'\n",
        "    OPTIMIZER = 'ADAM'\n",
        "\n",
        "    # Train on 1 GPU and 2 images per GPU.\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 2\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # background + TrafficLight    \n",
        "\n",
        "    # Backbone encoder architecture\n",
        "    BACKBONE = 'resnet101'\n",
        "\n",
        "    # Using smaller anchors because traffic lights  are small\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # How many anchors per image to use for RPN training\n",
        "    RPN_TRAIN_ANCHORS_PER_IMAGE = 320  #\n",
        "\n",
        "    # ROIs kept after non-maximum supression (training and inference)\n",
        "    POST_NMS_ROIS_TRAINING = 2048\n",
        "    POST_NMS_ROIS_INFERENCE = 2048\n",
        "\n",
        "    # Number of ROIs per image to feed to classifier/mask heads\n",
        "    TRAIN_ROIS_PER_IMAGE = 512\n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more proposals.\n",
        "    RPN_NMS_THRESHOLD = 0.7\n",
        "    # Maximum number of ground truth instances to use in one image\n",
        "    MAX_GT_INSTANCES = 256\n",
        "\n",
        "    # Max number of final detections\n",
        "    DETECTION_MAX_INSTANCES = 400\n",
        "\n",
        "    # Minimum probability value to accept a detected instance\n",
        "    # ROIs below this threshold are skipped\n",
        "    DETECTION_MIN_CONFIDENCE = 0.75\n",
        "\n",
        "    # Non-maximum suppression threshold for detection\n",
        "    DETECTION_NMS_THRESHOLD = 0.3  # 0.3\n",
        "\n",
        "    # Threshold number for mask binarization, only used in inference mode\n",
        "    DETECTION_MASK_THRESHOLD = 0.35\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlemt4Gk2gQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ############################################################\n",
        "# #  Dataset\n",
        "# ############################################################\n",
        "\n",
        "# class TrafficLightDataset(utils.Dataset):\n",
        "\n",
        "#     def load_trafficlight(self, dataset_dir, subset):\n",
        "#         \"\"\"Load a subset of the Balloon dataset.\n",
        "#         dataset_dir: Root directory of the dataset.\n",
        "#         subset: Subset to load: train or val\n",
        "#         \"\"\"\n",
        "#         # Add classes. We have only one class to add.\n",
        "#         self.add_class(\"trafficlight\", 1, \"trafficlight\")\n",
        "\n",
        "#         # Train or validation dataset?\n",
        "#         assert subset in [\"train\", \"val\"]\n",
        "#         dataset_dir = os.path.join(dataset_dir, subset)\n",
        "#         annotations = json.load(open(os.path.join(dataset_dir, \"via_region.json\")))\n",
        "        \n",
        "#         # Add images\n",
        "#         for a in annotations:\n",
        "#           rect = a[\"boxes\"]\n",
        "#           image_id = a[\"path\"].split('/')[4]\n",
        "#           image_path = os.path.join(dataset_dir, image_id)\n",
        "#           image = skimage.io.imread(image_path)\n",
        "#           height, width = image.shape[:2]\n",
        "#           self.add_image(\n",
        "#                 \"trafficlight\",\n",
        "#                 image_id=image_id,  # use file name as a unique image id\n",
        "#                 path=image_path,\n",
        "#                 width=width, height=height,\n",
        "#                 rect=rect)\n",
        "          \n",
        "#     def load_mask(self, image_id):\n",
        "#         \"\"\"Generate instance masks for an image.\n",
        "#        Returns:\n",
        "#         masks: A bool array of shape [height, width, instance count] with\n",
        "#             one mask per instance.\n",
        "#         class_ids: a 1D array of class IDs of the instance masks.\n",
        "#         \"\"\"\n",
        "#         # If not a trafficlight dataset image, delegate to parent class.\n",
        "#         image_info = self.image_info[image_id]\n",
        "#         if image_info[\"source\"] != \"trafficlight\":\n",
        "#             return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "#         # Convert rect to a bitmap mask of shape\n",
        "#         # [height, width, instance_count]\n",
        "#         if image_info[\"source\"] == \"trafficlight\":\n",
        "#           info = self.image_info[image_id]\n",
        "#           print(info)\n",
        "#           mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"rect\"])],dtype=np.uint8)\n",
        "#           print(type(mask)) \n",
        "#         for i, p in enumerate(info[\"rect\"]):\n",
        "#             # Get indexes of pixels inside the rect and set them to 1\n",
        "            \n",
        "#             start = (p[\"y_min\"], p[\"x_min\"])\n",
        "#             end = (p[\"y_max\"], p[\"x_max\"])\n",
        "#             print(\"START:\", start)\n",
        "#             print(\"END:\", end)\n",
        "#             rr, cc = skimage.draw.rectangle(start, end)\n",
        "#             rr = rr.astype(int)\n",
        "#             cc = cc.astype(int)\n",
        "#             mask[rr, cc, i] = 1\n",
        "\n",
        "#         return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
        "\n",
        "#     def image_reference(self, image_id):\n",
        "#         \"\"\"Return the path of the image.\"\"\"\n",
        "#         print(\"IN IMAGE REFERENCE ***************$$$$$$$$\")\n",
        "#         info = self.image_info[image_id]\n",
        "#         if info[\"source\"] == \"trafficlight\":\n",
        "#             return info[\"path\"]\n",
        "#         else:\n",
        "#             super(self.__class__, self).image_reference(image_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yiKu13n3Gni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  Dataset\n",
        "############################################################\n",
        "\n",
        "class TrafficLightDataset(utils.Dataset):\n",
        "\n",
        "    def load_trafficlight(self, df, train_path):\n",
        "        \"\"\"Load a subset of the Balloon dataset.\n",
        "        dataset_dir: Root directory of the dataset.\n",
        "        subset: Subset to load: train or val\n",
        "        \"\"\"\n",
        "        # Add classes. We have only one class to add.\n",
        "        self.add_class(\"trafficlight\", 1, \"trafficlight\")\n",
        "\n",
        "        # Train or validation dataset\n",
        "        #image_path = df['path'].split('/')[4]\n",
        "        #image = skimage.io.imread(image_path)\n",
        "        #height, width = image.shape[:2]\n",
        "        #print(image_path)\n",
        "        print(df.index)\n",
        "        for ind in df.index: \n",
        "          image_path = os.path.join(train_path, ind)\n",
        "          image = skimage.io.imread(image_path)\n",
        "          height, width = image.shape[:2]\n",
        "          self.add_image(\n",
        "                \"trafficlight\",\n",
        "                image_id=ind,  # use file name as a unique image id\n",
        "                path=image_path,\n",
        "                width=width, height=height,\n",
        "                rect=df['rect'][ind])\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a trafficlight dataset image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"trafficlight\":\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Convert rect to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "        if image_info[\"source\"] == \"trafficlight\":\n",
        "          info = self.image_info[image_id]\n",
        "          mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"rect\"])],dtype=np.uint8)\n",
        "          \n",
        "        for i, p in enumerate(info[\"rect\"]):\n",
        "            # Get indexes of pixels inside the rect and set them to 1\n",
        "            \n",
        "            start = (p[\"y_min\"], p[\"x_min\"])\n",
        "            end = (p[\"y_max\"], p[\"x_max\"])\n",
        "            rr, cc = skimage.draw.rectangle(start, end)\n",
        "            rr = rr.astype(int)\n",
        "            cc = cc.astype(int)\n",
        "            mask[rr, cc, i] = 1\n",
        "\n",
        "        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        print(\"IN IMAGE REFERENCE ***************$$$$$$$$\")\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"trafficlight\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ErD-1xAFMYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_validation_split(train_path, seed=10, test_size=0.1):\n",
        "\n",
        "    \"\"\"\n",
        "    Split the dataset into train and validation sets.\n",
        "    External data and mosaics are directly appended to training set.\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    df = pd.read_json(os.path.join(train_path, \"via_region.json\"))\n",
        "    print(df)\n",
        "    df['filename'] = df['path'].apply(lambda x: x.split('/')[4])\n",
        "    df['rect'] = df['boxes']\n",
        "    \n",
        "    df = df.set_index('filename')\n",
        "    train_list, val_list = train_test_split(df, test_size=test_size,\n",
        "                                            random_state=seed)\n",
        "    return train_list, val_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm0xbjGO6OxG",
        "colab_type": "code",
        "outputId": "6181032d-ef80-434c-e8ea-f3c897e0d3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "1\n",
        "6203\n",
        "36230.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  import time\n",
        "  start = time.time()\n",
        "\n",
        "  train_list, val_list = train_validation_split(\"/content/drive/My Drive/Colab Notebooks/HCLHackathon/RIBDATA/full\", seed=11, test_size=0.1)\n",
        "# Training dataset.\n",
        "  dataset_train = TrafficLightDataset()\n",
        "  dataset_train.load_trafficlight(train_list, \"/content/drive/My Drive/Colab Notebooks/HCLHackathon/RIBDATA/full\")\n",
        "  dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "  dataset_val = TrafficLightDataset()\n",
        "  dataset_val.load_trafficlight(val_list, \"/content/drive/My Drive/Colab Notebooks/HCLHackathon/RIBDATA/full\")\n",
        "  dataset_val.prepare()\n",
        "\n",
        "# Create model configuration in training mode\n",
        "  config = TrafficLightConfig()\n",
        "  config.STEPS_PER_EPOCH = len(train_list)//config.BATCH_SIZE\n",
        "  config.VALIDATION_STEPS = len(val_list)//config.BATCH_SIZE\n",
        "  config.display()\n",
        "\n",
        "  model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                              model_dir=DEFAULT_LOGS_DIR)\n",
        "\n",
        "# Model weights to start training with\n",
        "  init_with = \"pretrained\"  # imagenet, last, or some pretrained model\n",
        "\n",
        "  if init_with == \"imagenet\":\n",
        "    weights_path = \"/content/drive/My Drive/Colab Notebooks/HCLHackathon/logs/trafficlight20191128T1144/mask_rcnn_trafficlight_0050.h5\"\n",
        "    model.load_weights(weights_path, by_name=True, exclude=[\n",
        "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
        "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "  elif init_with == \"last\":\n",
        "  # Load the last model you trained and continue training\n",
        "    weights_path = \"/content/drive/My Drive/Colab Notebooks/HCLHackathon/logs/trafficlight20191128T1144/mask_rcnn_trafficlight_0050.h5\"\n",
        "    print('Loading weights from ', weights_path)\n",
        "    model.load_weights(weights_path, by_name=True)\n",
        "  elif init_with == 'pretrained':\n",
        "    weights_path = '/content/drive/My Drive/Colab Notebooks/HCLHackathon/logs/trafficlight20191128T1144/mask_rcnn_trafficlight_0075.h5'\n",
        "    model.load_weights(weights_path, by_name=True)\n",
        "  print('Loading weights from ', weights_path)\n",
        "\n",
        "  # Train the model for 75 epochs\n",
        "\n",
        "  #model.train(dataset_train, dataset_val,\n",
        "                # learning_rate=1e-4,\n",
        "                # epochs=25,\n",
        "                # layers='all')\n",
        "\n",
        "  # model.train(dataset_train, dataset_val,\n",
        "                # learning_rate=1e-5,\n",
        "                # epochs=50,\n",
        "                # layers='all')\n",
        "\n",
        "  model.train(dataset_train, dataset_val,\n",
        "                learning_rate=1e-6,\n",
        "                epochs=100,\n",
        "                layers='all')\n",
        "\n",
        "  print('Elapsed time', round((time.time() - start)/60, 1), 'minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 boxes                                               path\n",
            "0                                                   []  ./rgb/additional/2015-10-05-10-52-01_bag/24594...\n",
            "1                                                   []  ./rgb/additional/2015-10-05-10-52-01_bag/24664...\n",
            "2                                                   []  ./rgb/additional/2015-10-05-10-52-01_bag/24734...\n",
            "3    [{'label': 'Yellow', 'occluded': False, 'x_max...  ./rgb/additional/2015-10-05-10-55-33_bag/56988...\n",
            "4    [{'label': 'Red', 'occluded': False, 'x_max': ...  ./rgb/additional/2015-10-05-10-55-33_bag/57058...\n",
            "..                                                 ...                                                ...\n",
            "210  [{'label': 'Red', 'occluded': False, 'x_max': ...  ./rgb/additional/2015-10-05-16-02-30_bag/72051...\n",
            "211  [{'label': 'GreenStraight', 'occluded': False,...  ./rgb/additional/2015-10-05-16-02-30_bag/72058...\n",
            "212  [{'label': 'Green', 'occluded': False, 'x_max'...  ./rgb/additional/2015-10-05-16-02-30_bag/72072...\n",
            "213  [{'label': 'Green', 'occluded': False, 'x_max'...  ./rgb/additional/2015-10-05-16-02-30_bag/72079...\n",
            "214  [{'label': 'Green', 'occluded': False, 'x_max'...  ./rgb/additional/2015-10-05-16-02-30_bag/72086...\n",
            "\n",
            "[215 rows x 2 columns]\n",
            "Index(['24734.pgm', '61680.pgm', '719890.pgm', '536016.pgm', '622670.pgm',\n",
            "       '453032.pgm', '690882.pgm', '718500.pgm', '683266.pgm', '622740.pgm',\n",
            "       ...\n",
            "       '614636.pgm', '687268.pgm', '615192.pgm', '59526.pgm', '700144.pgm',\n",
            "       '620418.pgm', '622254.pgm', '620348.pgm', '713460.pgm', '685490.pgm'],\n",
            "      dtype='object', name='filename', length=193)\n",
            "Index(['680696.pgm', '707834.pgm', '614496.pgm', '687614.pgm', '690186.pgm',\n",
            "       '687128.pgm', '575176.pgm', '60986.pgm', '690048.pgm', '56988.pgm',\n",
            "       '625322.pgm', '622044.pgm', '624974.pgm', '684030.pgm', '60222.pgm',\n",
            "       '683822.pgm', '59666.pgm', '687058.pgm', '59804.pgm', '61332.pgm',\n",
            "       '630128.pgm', '24594.pgm'],\n",
            "      dtype='object', name='filename')\n",
            "\n",
            "Configurations:\n",
            "AUGMENT                        True\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     2\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MASK_THRESHOLD       0.35\n",
            "DETECTION_MAX_INSTANCES        400\n",
            "DETECTION_MIN_CONFIDENCE       0.75\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 2\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               256\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           TrafficLight\n",
            "NUM_CLASSES                    2\n",
            "OPTIMIZER                      ADAM\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        2048\n",
            "POST_NMS_ROIS_TRAINING         2048\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    320\n",
            "SCALE                          True\n",
            "STEPS_PER_EPOCH                96\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           512\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               11\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/HCLHackathon/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/HCLHackathon/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/HCLHackathon/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Re-starting from epoch 75\n",
            "Loading weights from  /content/drive/My Drive/Colab Notebooks/HCLHackathon/logs/trafficlight20191128T1144/mask_rcnn_trafficlight_0075.h5\n",
            "\n",
            "Starting at epoch 75. LR=1e-06\n",
            "\n",
            "Checkpoint Path: /content/drive/My Drive/Colab Notebooks/HCLHackathon/logs/trafficlight20191128T1144/mask_rcnn_trafficlight_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "conv1                  (Conv2D)\n",
            "bn_conv1               (BatchNorm)\n",
            "res2a_branch2a         (Conv2D)\n",
            "bn2a_branch2a          (BatchNorm)\n",
            "res2a_branch2b         (Conv2D)\n",
            "bn2a_branch2b          (BatchNorm)\n",
            "res2a_branch2c         (Conv2D)\n",
            "res2a_branch1          (Conv2D)\n",
            "bn2a_branch2c          (BatchNorm)\n",
            "bn2a_branch1           (BatchNorm)\n",
            "res2b_branch2a         (Conv2D)\n",
            "bn2b_branch2a          (BatchNorm)\n",
            "res2b_branch2b         (Conv2D)\n",
            "bn2b_branch2b          (BatchNorm)\n",
            "res2b_branch2c         (Conv2D)\n",
            "bn2b_branch2c          (BatchNorm)\n",
            "res2c_branch2a         (Conv2D)\n",
            "bn2c_branch2a          (BatchNorm)\n",
            "res2c_branch2b         (Conv2D)\n",
            "bn2c_branch2b          (BatchNorm)\n",
            "res2c_branch2c         (Conv2D)\n",
            "bn2c_branch2c          (BatchNorm)\n",
            "res3a_branch2a         (Conv2D)\n",
            "bn3a_branch2a          (BatchNorm)\n",
            "res3a_branch2b         (Conv2D)\n",
            "bn3a_branch2b          (BatchNorm)\n",
            "res3a_branch2c         (Conv2D)\n",
            "res3a_branch1          (Conv2D)\n",
            "bn3a_branch2c          (BatchNorm)\n",
            "bn3a_branch1           (BatchNorm)\n",
            "res3b_branch2a         (Conv2D)\n",
            "bn3b_branch2a          (BatchNorm)\n",
            "res3b_branch2b         (Conv2D)\n",
            "bn3b_branch2b          (BatchNorm)\n",
            "res3b_branch2c         (Conv2D)\n",
            "bn3b_branch2c          (BatchNorm)\n",
            "res3c_branch2a         (Conv2D)\n",
            "bn3c_branch2a          (BatchNorm)\n",
            "res3c_branch2b         (Conv2D)\n",
            "bn3c_branch2b          (BatchNorm)\n",
            "res3c_branch2c         (Conv2D)\n",
            "bn3c_branch2c          (BatchNorm)\n",
            "res3d_branch2a         (Conv2D)\n",
            "bn3d_branch2a          (BatchNorm)\n",
            "res3d_branch2b         (Conv2D)\n",
            "bn3d_branch2b          (BatchNorm)\n",
            "res3d_branch2c         (Conv2D)\n",
            "bn3d_branch2c          (BatchNorm)\n",
            "res4a_branch2a         (Conv2D)\n",
            "bn4a_branch2a          (BatchNorm)\n",
            "res4a_branch2b         (Conv2D)\n",
            "bn4a_branch2b          (BatchNorm)\n",
            "res4a_branch2c         (Conv2D)\n",
            "res4a_branch1          (Conv2D)\n",
            "bn4a_branch2c          (BatchNorm)\n",
            "bn4a_branch1           (BatchNorm)\n",
            "res4b_branch2a         (Conv2D)\n",
            "bn4b_branch2a          (BatchNorm)\n",
            "res4b_branch2b         (Conv2D)\n",
            "bn4b_branch2b          (BatchNorm)\n",
            "res4b_branch2c         (Conv2D)\n",
            "bn4b_branch2c          (BatchNorm)\n",
            "res4c_branch2a         (Conv2D)\n",
            "bn4c_branch2a          (BatchNorm)\n",
            "res4c_branch2b         (Conv2D)\n",
            "bn4c_branch2b          (BatchNorm)\n",
            "res4c_branch2c         (Conv2D)\n",
            "bn4c_branch2c          (BatchNorm)\n",
            "res4d_branch2a         (Conv2D)\n",
            "bn4d_branch2a          (BatchNorm)\n",
            "res4d_branch2b         (Conv2D)\n",
            "bn4d_branch2b          (BatchNorm)\n",
            "res4d_branch2c         (Conv2D)\n",
            "bn4d_branch2c          (BatchNorm)\n",
            "res4e_branch2a         (Conv2D)\n",
            "bn4e_branch2a          (BatchNorm)\n",
            "res4e_branch2b         (Conv2D)\n",
            "bn4e_branch2b          (BatchNorm)\n",
            "res4e_branch2c         (Conv2D)\n",
            "bn4e_branch2c          (BatchNorm)\n",
            "res4f_branch2a         (Conv2D)\n",
            "bn4f_branch2a          (BatchNorm)\n",
            "res4f_branch2b         (Conv2D)\n",
            "bn4f_branch2b          (BatchNorm)\n",
            "res4f_branch2c         (Conv2D)\n",
            "bn4f_branch2c          (BatchNorm)\n",
            "res4g_branch2a         (Conv2D)\n",
            "bn4g_branch2a          (BatchNorm)\n",
            "res4g_branch2b         (Conv2D)\n",
            "bn4g_branch2b          (BatchNorm)\n",
            "res4g_branch2c         (Conv2D)\n",
            "bn4g_branch2c          (BatchNorm)\n",
            "res4h_branch2a         (Conv2D)\n",
            "bn4h_branch2a          (BatchNorm)\n",
            "res4h_branch2b         (Conv2D)\n",
            "bn4h_branch2b          (BatchNorm)\n",
            "res4h_branch2c         (Conv2D)\n",
            "bn4h_branch2c          (BatchNorm)\n",
            "res4i_branch2a         (Conv2D)\n",
            "bn4i_branch2a          (BatchNorm)\n",
            "res4i_branch2b         (Conv2D)\n",
            "bn4i_branch2b          (BatchNorm)\n",
            "res4i_branch2c         (Conv2D)\n",
            "bn4i_branch2c          (BatchNorm)\n",
            "res4j_branch2a         (Conv2D)\n",
            "bn4j_branch2a          (BatchNorm)\n",
            "res4j_branch2b         (Conv2D)\n",
            "bn4j_branch2b          (BatchNorm)\n",
            "res4j_branch2c         (Conv2D)\n",
            "bn4j_branch2c          (BatchNorm)\n",
            "res4k_branch2a         (Conv2D)\n",
            "bn4k_branch2a          (BatchNorm)\n",
            "res4k_branch2b         (Conv2D)\n",
            "bn4k_branch2b          (BatchNorm)\n",
            "res4k_branch2c         (Conv2D)\n",
            "bn4k_branch2c          (BatchNorm)\n",
            "res4l_branch2a         (Conv2D)\n",
            "bn4l_branch2a          (BatchNorm)\n",
            "res4l_branch2b         (Conv2D)\n",
            "bn4l_branch2b          (BatchNorm)\n",
            "res4l_branch2c         (Conv2D)\n",
            "bn4l_branch2c          (BatchNorm)\n",
            "res4m_branch2a         (Conv2D)\n",
            "bn4m_branch2a          (BatchNorm)\n",
            "res4m_branch2b         (Conv2D)\n",
            "bn4m_branch2b          (BatchNorm)\n",
            "res4m_branch2c         (Conv2D)\n",
            "bn4m_branch2c          (BatchNorm)\n",
            "res4n_branch2a         (Conv2D)\n",
            "bn4n_branch2a          (BatchNorm)\n",
            "res4n_branch2b         (Conv2D)\n",
            "bn4n_branch2b          (BatchNorm)\n",
            "res4n_branch2c         (Conv2D)\n",
            "bn4n_branch2c          (BatchNorm)\n",
            "res4o_branch2a         (Conv2D)\n",
            "bn4o_branch2a          (BatchNorm)\n",
            "res4o_branch2b         (Conv2D)\n",
            "bn4o_branch2b          (BatchNorm)\n",
            "res4o_branch2c         (Conv2D)\n",
            "bn4o_branch2c          (BatchNorm)\n",
            "res4p_branch2a         (Conv2D)\n",
            "bn4p_branch2a          (BatchNorm)\n",
            "res4p_branch2b         (Conv2D)\n",
            "bn4p_branch2b          (BatchNorm)\n",
            "res4p_branch2c         (Conv2D)\n",
            "bn4p_branch2c          (BatchNorm)\n",
            "res4q_branch2a         (Conv2D)\n",
            "bn4q_branch2a          (BatchNorm)\n",
            "res4q_branch2b         (Conv2D)\n",
            "bn4q_branch2b          (BatchNorm)\n",
            "res4q_branch2c         (Conv2D)\n",
            "bn4q_branch2c          (BatchNorm)\n",
            "res4r_branch2a         (Conv2D)\n",
            "bn4r_branch2a          (BatchNorm)\n",
            "res4r_branch2b         (Conv2D)\n",
            "bn4r_branch2b          (BatchNorm)\n",
            "res4r_branch2c         (Conv2D)\n",
            "bn4r_branch2c          (BatchNorm)\n",
            "res4s_branch2a         (Conv2D)\n",
            "bn4s_branch2a          (BatchNorm)\n",
            "res4s_branch2b         (Conv2D)\n",
            "bn4s_branch2b          (BatchNorm)\n",
            "res4s_branch2c         (Conv2D)\n",
            "bn4s_branch2c          (BatchNorm)\n",
            "res4t_branch2a         (Conv2D)\n",
            "bn4t_branch2a          (BatchNorm)\n",
            "res4t_branch2b         (Conv2D)\n",
            "bn4t_branch2b          (BatchNorm)\n",
            "res4t_branch2c         (Conv2D)\n",
            "bn4t_branch2c          (BatchNorm)\n",
            "res4u_branch2a         (Conv2D)\n",
            "bn4u_branch2a          (BatchNorm)\n",
            "res4u_branch2b         (Conv2D)\n",
            "bn4u_branch2b          (BatchNorm)\n",
            "res4u_branch2c         (Conv2D)\n",
            "bn4u_branch2c          (BatchNorm)\n",
            "res4v_branch2a         (Conv2D)\n",
            "bn4v_branch2a          (BatchNorm)\n",
            "res4v_branch2b         (Conv2D)\n",
            "bn4v_branch2b          (BatchNorm)\n",
            "res4v_branch2c         (Conv2D)\n",
            "bn4v_branch2c          (BatchNorm)\n",
            "res4w_branch2a         (Conv2D)\n",
            "bn4w_branch2a          (BatchNorm)\n",
            "res4w_branch2b         (Conv2D)\n",
            "bn4w_branch2b          (BatchNorm)\n",
            "res4w_branch2c         (Conv2D)\n",
            "bn4w_branch2c          (BatchNorm)\n",
            "res5a_branch2a         (Conv2D)\n",
            "bn5a_branch2a          (BatchNorm)\n",
            "res5a_branch2b         (Conv2D)\n",
            "bn5a_branch2b          (BatchNorm)\n",
            "res5a_branch2c         (Conv2D)\n",
            "res5a_branch1          (Conv2D)\n",
            "bn5a_branch2c          (BatchNorm)\n",
            "bn5a_branch1           (BatchNorm)\n",
            "res5b_branch2a         (Conv2D)\n",
            "bn5b_branch2a          (BatchNorm)\n",
            "res5b_branch2b         (Conv2D)\n",
            "bn5b_branch2b          (BatchNorm)\n",
            "res5b_branch2c         (Conv2D)\n",
            "bn5b_branch2c          (BatchNorm)\n",
            "res5c_branch2a         (Conv2D)\n",
            "bn5c_branch2a          (BatchNorm)\n",
            "res5c_branch2b         (Conv2D)\n",
            "bn5c_branch2b          (BatchNorm)\n",
            "res5c_branch2c         (Conv2D)\n",
            "bn5c_branch2c          (BatchNorm)\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 76/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F24DNQcUPX2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJgzyuROPlbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BpQKkBFTvrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNXy4b652Y-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlWy2jPq7UxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1D622CKVQJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}